---
layout: default
title: Science Trends
---
<style>
#wrap { width: 1200px; height: 390px; padding: 0; overflow: hidden; }
#frame { width: 800px; height: 520px; border: 1px solid black; }
#frame { zoom: 0.75; -moz-transform: scale(0.55); -moz-transform-origin: 0 0; }

#outer{
max-width: 640px;
max-height: 360px;
width: 100%;
height: 100%;
}

#inner{
    height: 0px;
    padding-bottom: 56.25%;
}

#inner iframe{
    width: 40%;
    height: 40%;
}

</style>

<body>
	<div class="blurb">
	<h1>Science Trends</h1>		
	<h2>An Interactive Explorer of arXiv Submissions</h2>
		As a researcher (both in physics and datascience), a portion of my job description is to stay abreast of current trends in the field.
		With the volumes of papers published each year, it becomes nearly a full-time job to just keep up with my own domain, let alone beyond.
		Indeed, <a href="https://arxiv.org/help/stats/2015_by_area/index" target="_blank"> the rate of article submission to the arXiv</a> has continually grown since its launch in 1996. 
		In the quantum physics sub-repository, over 5000 submissions were received in 2015 alone.
		</br></br>
		I've long dreamt of a miracle tool that would automatically parse the vast libraries of scientific literature to
		distill out trends such as the emergence of new and exciting topics.  With the development of natural language processing techniques and the
		arXiv's participation in the <a href="https://arxiv.org/help/oa/index"  target="_blank">Open Access Initiative</a>, such a tool may be realisable.
		</br></br>
		This project is a first foray into the development of such a tool.  It has a much more modest goal of tracking scientific 
		trends as represented by the scientific literature.  In particular, I focus on my own domain of quantum physics.
		<!--
		</br></br>
		<p align="justify">
			Try out the word-frequency version of the <a href="#app1" target="_blank">App</a> 			
		</p>		
		-->
		
	</div>

	<div>
		</br></br>
		<h2>Data source</h2>
		The arXiv participates in the <a href="https://arxiv.org/help/oa/index"  target="_blank">Open Access Initiative</a>, allowing for bulk download of article metadata.
		The metadata for a submission consists of supporting information, such as:
		<ol>
			<li>Article title</li>
			<li>Author list </li>
			<li>Submission date </li>
			<li>Subject keywords</li>
			<li>Final publication location </li>
			<li>Article abstract </li>
		</ol>
		This information is encoded in XML format.  Using their harvesting protocol is simple:
		<pre><code>url = 'http://export.arxiv.org/oai2?verb=ListRecords&set=physics:quant-ph&metadataPrefix=oai_dc&{}'.format(queryDate)</br>rawData = urllib.urlopen(url).read()</code></pre>
		The above call will return up to 1000 results at a time.  A resumption token is provided at the end of the return if there are more than 1000 records available.
		This makes it possible to process large requests.
		
		<pre><code>url = 'http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken={}'.format(token)	</code></pre>

		The data I focus on here is the article title, abstract and the date of submission.  

		The text data is cleaned up a bit by removing non alpha numeric characters (such as latex code) and common stopwords.  These operations are
		performed using <i>obo</i>; many other packages do the same thing, for example <i>NLTK</i>.
		<pre><code>text_stripped = obo.stripNonAlphaNum(text)</br>text_cleaned = obo.removeStopwords(text_stripped,obo.stopwords)</code></pre>
		Some effort was put towards lemmatisation, but in the end I found better results without it.

		</br></br>
		The cleaned text is a <i>document</i>, and the complete collection of documents is referred to as a <i>corpus</i>.
	</div>			

	<div>
		</br></br>
		<h2>Approach 1: Word frequency analysis</h2>
		A simple approach to extracting trends from the data is to simply look at the frequency of keywords as a function of time.	

		To obtain time-dynamics, the data is grouped into quarterly blocks (3 months), and the most frequently occurring words in each quarter are identified.  Here I choose to keep only the top 5000 words.
		<pre><code>doc_dictionary = obo.wordListToFreqDict(doc)</br>doc_sorteddict = obo.sortFreqDict(doc_dictionary)</br>topWords = doc_sorteddict[:5000])</code></pre>

		Scores are generated by normalizing the word frequency by the number of submissions per quarter.  The resulting score is thus a measure of the relative significance of a subject.

		</br></br>
		<p id="app1">
		In the <a href="http://arxiv-word-app.herokuapp.com/" target="_blank">App</a>, you can browse a number of curated topics grouped by 
		theme.  One of my favorite themes is <i>Breakthroughs</i> which highlight some landmark publications that generated a significant and sustained
		level of interest in a new topic.  You can also freely explore the data, entering your own topic key words.  

		</br></br>
		<i>Note that the app is currently hosted on Heroku.  Please be patient as it takes a minute for the Heroku dynos to start up if they've been asleep.  
		A refresh or two may be necessary.  Migration to AWS coming soon!</i>		
		</p>
		
	</div>

	<div>
		</br></br>
		<h2>Approach 2: Latent semantic indexing</h2>
		A more sophisticated approach is to vectorize the corpus, providing a mathematical representation of the information.  A number of different
		transformations can be applied to the corpus which highlight different aspects of the content/

		These transformations and manipulations are performed with <a href="https://radimrehurek.com/gensim/"> <i>Gensim</i></a>.  The developers of Gensim have posted
		some great tutorials on how to use the package.
		</br></br>
		There are three commonly used transformations that can be applied:
		<ol>
			<li>Documents can be converted into a bag-of-words representation, that simply counts the frequency of each word.</li>
			<pre><code>bow = dictionary.doc2bow(corpus)</code></pre>

			<li>A slightly more sophisticated representation is <a href="https://en.wikipedia.org/wiki/Tfâ€“idf">Tf-Idf</a>, which assigns a weight to each word based on its relative importance.</li>
			<pre><code>tfidf = models.TfidfModel(corpus)</br>corpus_tfidf = tfidf[corpus]</code></pre>

			<li>The Tf-Idf representation can be transformed into a new space of topic vectors using singular value decomposition, an approach 
					referred to as Latent Semantic Indexing (LSI).  These topic vectors consist of linear combinations of the original dictionary words.					
					<pre><code>lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=Nfeat)</br>corpus_lsi = lsi[corpus_tfidf]</code></pre>
					The singular values associated with these topics additionally represent the relative importance of a topic to the corpus.  With this
					importance measure, dimensionality reduction can be performed, keeping only the most significant topics. The result is a light-weight 
					representation of the full corpus.					
			</li>
		</ol>
		The LSI approach is interesting since it lets the data (corpus) identify the important topics.  Unfortunately, these topic vectors
		are notoriously difficult to interpret.  Nonetheless, further work along this line seems like the way to have an unsupervised approach to 
		identifying new and important trends.

		In addition to the promise of unsupervised learning, the vectorization of the corpus combined with the dimensionality reduction, allow for the efficient 
		evaluation of document similarity for categorisation.  The most common similarity measure is a cosine similarity, which evaluates the inner product of two document vectors.

		</br></br> 		
			The user interface for this app is still in development, but the source code for the implementation is available on 
			<a href="https://github.com/rocksonchang/arXiv-metadata">GitHub</a>.
	</div>

	<div>
		</br></br>
		<h2>Summary</h2>
		Natural language processing (NLP) techniques allow for efficient analysis of large bodies of text.  A variety of approaches are available, just 
		a few of which have been touched on here (for example, I haven't even touched the subject of sentiment analysis).
		
		</br></br>
		Considering the bulk of information stored as text, both in more traditional print media (digitisable with 
		<a href="https://en.wikipedia.org/wiki/Optical_character_recognition" target="_blank">OCR</a> techniques) and modern 
		digital media, there are no lack of data sources and range of domains where such NLP approaches may have a significant impact.

		</br></br>
		The above project represents a first foray into the exploration of the trends with.  Evidently, with the richness of the
		dataset, a wide variety of features can be explored.  One such goal is to try and detect the emergence of new subjects before
		they explode without having domain knowledge. 
	</div>
	
	<!--
	dictionary = corpora.Dictionary.load(path)
	corpus = [dictionary.doc2bow(text) for text in texts]

	bow = dictionary.doc2bow(desc)

	tfidf = models.TfidfModel(corpus)
	corpus_tfidf = tfidf[corpus]
	!-->

	
	<!-- 
	<div id="outer">
		<div id="inner">
			<iframe src="http://arxiv-word-app.herokuapp.com/index" style="margin: 0 auto;display:block;float:left; width: 920px; height: 1200px; -webkit-transform:scale(0.90);"></iframe>
			<iframe src="http://arxiv-word-app.herokuapp.com/index" style="width: 900px; height: 1200px" ></iframe>	
		</div>
	</div>
	-->
	
	<div style="clear:both;"></div>
	</br></br> 
	<div class="line" style="padding: 0 0 0 20px;""></div>
	Source code and files for the above projects are available on <a href="https://github.com/rocksonchang/Science-Trends">GitHub</a>.
	<div>
		<p>
		<br>			
		
		</p>	
	</div>

</body>
